{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9840, 2048, 3)\n",
      "(9840, 1)\n",
      "(2468, 2048, 3)\n",
      "(2468, 1)\n"
     ]
    }
   ],
   "source": [
    "data_path = './ModelNet40/'\n",
    "\n",
    "for d in [['train', len(os.listdir(data_path + 'train'))], ['test', len(os.listdir(data_path + 'test'))]]:\n",
    "    data = None\n",
    "    labels = None\n",
    "    for j in range(d[1]):\n",
    "        file_name = data_path + d[0] + '/ply_data_{0}{1}.h5'.format(d[0], j)\n",
    "        f = h5py.File(file_name, mode='r')\n",
    "        if data is None:\n",
    "            data = f['data']\n",
    "            labels = f['label']\n",
    "        else:\n",
    "            data = np.vstack((data, f['data']))\n",
    "            labels = np.vstack((labels, f['label']))\n",
    "    f.close()\n",
    "    save_name = data_path + '/ply_data_{0}.h5'.format(d[0])\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    h5_fout = h5py.File(save_name)\n",
    "    h5_fout.create_dataset(\n",
    "        'data', data=data,\n",
    "        dtype='float32')\n",
    "    h5_fout.create_dataset(\n",
    "        'label', data=labels,\n",
    "        dtype='float32')\n",
    "    h5_fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 40\n",
    "train_file = './ModelNet40/ply_data_train.h5'\n",
    "test_file = './ModelNet40/ply_data_test.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, file_name, batch_size, nb_classes=40, train=True):\n",
    "        self.fie_name = file_name\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.train = train\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_point_cloud(data):\n",
    "        \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "            rotation is per shape based along up direction\n",
    "            Input:\n",
    "              Nx3 array, original point clouds\n",
    "            Return:\n",
    "              Nx3 array, rotated point clouds\n",
    "        \"\"\"\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        rotated_data = np.dot(data.reshape((-1, 3)), rotation_matrix)\n",
    "        return rotated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def jitter_point_cloud(data, sigma=0.01, clip=0.05):\n",
    "        \"\"\" Randomly jitter points. jittering is per point.\n",
    "            Input:\n",
    "              Nx3 array, original point clouds\n",
    "            Return:\n",
    "              Nx3 array, jittered point clouds\n",
    "        \"\"\"\n",
    "        N, C = data.shape\n",
    "        assert (clip > 0)\n",
    "        jittered_data = np.clip(sigma * np.random.randn(N, C), -1 * clip, clip)\n",
    "        jittered_data += data\n",
    "        return jittered_data\n",
    "\n",
    "    def generator(self):\n",
    "        f = h5py.File(self.fie_name, mode='r')\n",
    "        nb_sample = f['data'].shape[0]\n",
    "        while True:\n",
    "            index = [n for n in range(nb_sample)]\n",
    "            random.shuffle(index)\n",
    "            for i in range(nb_sample // self.batch_size):\n",
    "                batch_start = i * self.batch_size\n",
    "                batch_end = (i + 1) * self.batch_size\n",
    "                batch_index = index[batch_start: batch_end]\n",
    "                X = []\n",
    "                Y = []\n",
    "                for j in batch_index:\n",
    "                    item = f['data'][j]\n",
    "                    label = f['label'][j]\n",
    "                    if self.train:\n",
    "                        is_rotate = random.randint(0, 1)\n",
    "                        is_jitter = random.randint(0, 1)\n",
    "                        if is_rotate == 1:\n",
    "                            item = self.rotate_point_cloud(item)\n",
    "                        if is_jitter == 1:\n",
    "                            item = self.jitter_point_cloud(item)\n",
    "                    X.append(item)\n",
    "                    Y.append(label[0])\n",
    "                Y = np_utils.to_categorical(np.array(Y), self.nb_classes)\n",
    "                yield np.array(X), Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataGenerator(train_file, batch_size, nb_classes, train=True)\n",
    "val = DataGenerator(test_file, batch_size, nb_classes, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Input, BatchNormalization, Dense\n",
    "from keras.layers import Reshape, Lambda, concatenate\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMul, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Used purely for shape validation.\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('`MatMul` layer should be called '\n",
    "                             'on a list of inputs')\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError('The input of `MatMul` layer should be a list containing 2 elements')\n",
    "\n",
    "        if len(input_shape[0]) != 3 or len(input_shape[1]) != 3:\n",
    "            raise ValueError('The dimensions of each element of inputs should be 3')\n",
    "\n",
    "        if input_shape[0][-1] != input_shape[1][1]:\n",
    "            raise ValueError('The last dimension of inputs[0] should match the dimension 1 of inputs[1]')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            raise ValueError('A `MatMul` layer should be called '\n",
    "                             'on a list of inputs.')\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = [input_shape[0][0], input_shape[0][1], input_shape[1][-1]]\n",
    "        return tuple(output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointNet(nb_classes):\n",
    "    input_points = Input(shape=(2048, 3))\n",
    "    # issues\n",
    "    # input transformation net\n",
    "    x = Conv1D(64, 1, activation='relu')(input_points)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(128, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(1024, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2048)(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "    input_T = Reshape((3, 3))(x)\n",
    "\n",
    "    # forward net\n",
    "    g = MatMul()([input_points, input_T])\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "    # feature transform net\n",
    "    f = Conv1D(64, 1, activation='relu')(g)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Conv1D(128, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Conv1D(1024, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = MaxPooling1D(pool_size=2048)(f)\n",
    "    f = Dense(512, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(256, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "    feature_T = Reshape((64, 64))(f)\n",
    "\n",
    "    # forward net\n",
    "    g = MatMul()([g, feature_T])\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(128, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(1024, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "    # global feature\n",
    "    global_feature = MaxPooling1D(pool_size=2048)(g)\n",
    "\n",
    "    # point_net_cls\n",
    "    c = Dense(512, activation='relu')(global_feature)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    c = Dense(256, activation='relu')(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    c = Dense(nb_classes, activation='softmax')(c)\n",
    "    prediction = Flatten()(c)\n",
    "\n",
    "    model = Model(inputs=input_points, outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointNet(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 2048, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 2048, 64)     256         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 2048, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2048, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2048, 1024)   4096        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 1024)      0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 512)       524800      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1, 512)       2048        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 256)       131328      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1, 256)       1024        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 9)         2313        batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 3, 3)         0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_4 (MatMul)              (None, 2048, 3)      0           input_4[0][0]                    \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 2048, 64)     256         mat_mul_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2048, 64)     256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 2048, 64)     4160        batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2048, 64)     256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 2048, 64)     4160        batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2048, 64)     256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2048, 128)    512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2048, 1024)   4096        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 1024)      0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1, 512)       524800      max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 1, 512)       2048        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1, 256)       131328      batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1, 256)       1024        dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1, 4096)      1052672     batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 64, 64)       0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_5 (MatMul)              (None, 2048, 64)     0           batch_normalization_34[0][0]     \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 2048, 64)     4160        mat_mul_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 2048, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 2048, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2048, 1024)   4096        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 1024)      0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 512)       524800      max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1, 512)       2048        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 512)       0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1, 256)       131328      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 1, 256)       1024        dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 256)       0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1, 40)        10280       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 40)           0           dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,492,209\n",
      "Trainable params: 3,480,049\n",
      "Non-trainable params: 12,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "adam = Adam(lr=lr)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./results/'):\n",
    "    os.mkdir('./results/')\n",
    "checkpoint = ModelCheckpoint('./results/pointnet.h5', monitor='val_acc',\n",
    "                             save_weights_only=True, save_best_only=True,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(Callback):\n",
    "\n",
    "    def __init__(self, steps, learning_rates, verbose=0):\n",
    "        self.steps = steps\n",
    "        self.lr = learning_rates\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def change_lr(self, new_lr):\n",
    "        old_lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "        if self.verbose == 1:\n",
    "            print('Learning rate is %g' %new_lr)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        for i, step in enumerate(self.steps):\n",
    "            if epoch < step:\n",
    "                self.change_lr(self.lr[i])\n",
    "                return\n",
    "        self.change_lr(self.lr[i+1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'class': type(self).__name__,\n",
    "                  'steps': self.steps,\n",
    "                  'learning_rates': self.lr,\n",
    "                  'verbose': self.verbose}\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        offset = config.get('epoch_offset', 0)\n",
    "        steps = [step - offset for step in config['steps']]\n",
    "        return cls(steps, config['learning_rates'],\n",
    "                   verbose=config.get('verbose', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetenth_50_75(lr):\n",
    "    steps = [50, 75]\n",
    "    lrs = [lr, lr / 10, lr / 100]\n",
    "    return Step(steps, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "122/307 [==========>...................] - ETA: 21:43 - loss: 3.9090 - acc: 0.1673"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train.generator(),\n",
    "                              steps_per_epoch=9840 // batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val.generator(),\n",
    "                              validation_steps=2468 // batch_size,\n",
    "                              callbacks=[checkpoint, onetenth_50_75(lr)],\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, result_dir):\n",
    "    plt.plot(history.history['acc'], marker='.')\n",
    "    plt.plot(history.history['val_acc'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend(['acc', 'val_acc'], loc='lower right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_accuracy.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_loss.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, result_dir):\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    nb_epoch = len(acc)\n",
    "\n",
    "    with open(os.path.join(result_dir, 'result.txt'), 'w') as fp:\n",
    "        fp.write('epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n')\n",
    "        for i in range(nb_epoch):\n",
    "            fp.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                i, loss[i], acc[i], val_loss[i], val_acc[i]))\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, './results/')\n",
    "save_history(history, './results/')\n",
    "model.save_weights('./results/pointnet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
