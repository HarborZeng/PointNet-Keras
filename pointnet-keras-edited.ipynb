{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9840, 2048, 3)\n",
      "(9840, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (no write intent on file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-515decbb3c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     h5_fout.create_dataset(\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         dtype='float32')\n\u001b[0m\u001b[1;32m     23\u001b[0m     h5_fout.create_dataset(\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mdset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create dataset (no write intent on file)"
     ]
    }
   ],
   "source": [
    "data_path = './ModelNet40/'\n",
    "\n",
    "for d in [['train', len(os.listdir(data_path + 'train'))], ['test', len(os.listdir(data_path + 'test'))]]:\n",
    "    data = None\n",
    "    labels = None\n",
    "    for j in range(d[1]):\n",
    "        file_name = data_path + d[0] + '/ply_data_{0}{1}.h5'.format(d[0], j)\n",
    "        f = h5py.File(file_name, mode='r')\n",
    "        if data is None:\n",
    "            data = f['data']\n",
    "            labels = f['label']\n",
    "        else:\n",
    "            data = np.vstack((data, f['data']))\n",
    "            labels = np.vstack((labels, f['label']))\n",
    "    f.close()\n",
    "    save_name = data_path + '/ply_data_{0}.h5'.format(d[0])\n",
    "    print(data.shape)\n",
    "    print(labels.shape)\n",
    "    h5_fout = h5py.File(save_name)\n",
    "    h5_fout.create_dataset(\n",
    "        'data', data=data,\n",
    "        dtype='float32')\n",
    "    h5_fout.create_dataset(\n",
    "        'label', data=labels,\n",
    "        dtype='float32')\n",
    "    h5_fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 40\n",
    "train_file = './ModelNet40/train/ply_data_train0.h5'\n",
    "test_file = './ModelNet40/test/ply_data_test1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "class DataGenerator:\n",
    "    def __init__(self, file_name, batch_size, nb_classes=40, train=True):\n",
    "        self.fie_name = file_name\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.train = train\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_point_cloud(data):\n",
    "        \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "            rotation is per shape based along up direction\n",
    "            Input:\n",
    "              Nx3 array, original point clouds\n",
    "            Return:\n",
    "              Nx3 array, rotated point clouds\n",
    "        \"\"\"\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        rotated_data = np.dot(data.reshape((-1, 3)), rotation_matrix)\n",
    "        return rotated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def jitter_point_cloud(data, sigma=0.01, clip=0.05):\n",
    "        \"\"\" Randomly jitter points. jittering is per point.\n",
    "            Input:\n",
    "              Nx3 array, original point clouds\n",
    "            Return:\n",
    "              Nx3 array, jittered point clouds\n",
    "        \"\"\"\n",
    "        N, C = data.shape\n",
    "        assert (clip > 0)\n",
    "        jittered_data = np.clip(sigma * np.random.randn(N, C), -1 * clip, clip)\n",
    "        jittered_data += data\n",
    "        return jittered_data\n",
    "\n",
    "    def generator(self):\n",
    "        f = h5py.File(self.fie_name, mode='r')\n",
    "        nb_sample = f['data'].shape[0]\n",
    "        while True:\n",
    "            index = [n for n in range(nb_sample)]\n",
    "            random.shuffle(index)\n",
    "            for i in range(nb_sample // self.batch_size):\n",
    "                batch_start = i * self.batch_size\n",
    "                batch_end = (i + 1) * self.batch_size\n",
    "                batch_index = index[batch_start: batch_end]\n",
    "                X = []\n",
    "                Y = []\n",
    "                for j in batch_index:\n",
    "                    item = f['data'][j]\n",
    "                    label = f['label'][j]\n",
    "                    if self.train:\n",
    "                        is_rotate = random.randint(0, 1)\n",
    "                        is_jitter = random.randint(0, 1)\n",
    "                        if is_rotate == 1:\n",
    "                            item = self.rotate_point_cloud(item)\n",
    "                        if is_jitter == 1:\n",
    "                            item = self.jitter_point_cloud(item)\n",
    "                    X.append(item)\n",
    "                    Y.append(label[0])\n",
    "                Y = np_utils.to_categorical(np.array(Y), self.nb_classes)\n",
    "                yield np.array(X), Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataGenerator(train_file, batch_size, nb_classes, train=True)\n",
    "val = DataGenerator(test_file, batch_size, nb_classes, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Input, BatchNormalization, Dense\n",
    "from keras.layers import Reshape, Lambda, concatenate\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMul, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Used purely for shape validation.\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('`MatMul` layer should be called '\n",
    "                             'on a list of inputs')\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError('The input of `MatMul` layer should be a list containing 2 elements')\n",
    "\n",
    "        if len(input_shape[0]) != 3 or len(input_shape[1]) != 3:\n",
    "            raise ValueError('The dimensions of each element of inputs should be 3')\n",
    "\n",
    "        if input_shape[0][-1] != input_shape[1][1]:\n",
    "            raise ValueError('The last dimension of inputs[0] should match the dimension 1 of inputs[1]')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            raise ValueError('A `MatMul` layer should be called '\n",
    "                             'on a list of inputs.')\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = [input_shape[0][0], input_shape[0][1], input_shape[1][-1]]\n",
    "        return tuple(output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointNet(nb_classes):\n",
    "    input_points = Input(shape=(2048, 3))\n",
    "    # issues\n",
    "    # input transformation net\n",
    "    x = Conv1D(64, 1, activation='relu')(input_points)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(128, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(1024, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2048)(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "    input_T = Reshape((3, 3))(x)\n",
    "\n",
    "    # forward net\n",
    "    g = MatMul()([input_points, input_T])\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "    # feature transform net\n",
    "    f = Conv1D(64, 1, activation='relu')(g)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Conv1D(128, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Conv1D(1024, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = MaxPooling1D(pool_size=2048)(f)\n",
    "    f = Dense(512, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(256, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "    feature_T = Reshape((64, 64))(f)\n",
    "\n",
    "    # forward net\n",
    "    g = MatMul()([g, feature_T])\n",
    "    g = Conv1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(128, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Conv1D(1024, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "    # global feature\n",
    "    global_feature = MaxPooling1D(pool_size=2048)(g)\n",
    "\n",
    "    # point_net_cls\n",
    "    c = Dense(512, activation='relu')(global_feature)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    c = Dense(256, activation='relu')(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    c = Dense(nb_classes, activation='softmax')(c)\n",
    "    prediction = Flatten()(c)\n",
    "\n",
    "    model = Model(inputs=input_points, outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointNet(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 2048, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 2048, 64)     256         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 2048, 64)     256         conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 2048, 128)    512         conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 2048, 1024)   4096        conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 1, 1024)      0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 1, 512)       524800      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 1, 512)       2048        dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 1, 256)       131328      batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 1, 256)       1024        dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 1, 9)         2313        batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 3, 3)         0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_15 (MatMul)             (None, 2048, 3)      0           input_8[0][0]                    \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 2048, 64)     256         mat_mul_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 2048, 64)     256         conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 2048, 64)     4160        batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 2048, 64)     256         conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 2048, 64)     4160        batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 2048, 64)     256         conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 2048, 128)    512         conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 2048, 1024)   4096        conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 1, 1024)      0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 1, 512)       524800      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 1, 512)       2048        dense_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 1, 256)       131328      batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 1, 256)       1024        dense_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 1, 4096)      1052672     batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 64, 64)       0           dense_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_16 (MatMul)             (None, 2048, 64)     0           batch_normalization_126[0][0]    \n",
      "                                                                 reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 2048, 64)     4160        mat_mul_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 2048, 64)     256         conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 2048, 128)    8320        batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 2048, 128)    512         conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 2048, 1024)   132096      batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 2048, 1024)   4096        conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 1, 1024)      0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 1, 512)       524800      max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 1, 512)       2048        dense_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 512)       0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 1, 256)       131328      dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 1, 256)       1024        dense_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1, 256)       0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 1, 40)        10280       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 40)           0           dense_72[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,492,209\n",
      "Trainable params: 3,480,049\n",
      "Non-trainable params: 12,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "adam = Adam(lr=lr)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaCheckpoint(ModelCheckpoint):\n",
    "    \"\"\"\n",
    "    Checkpoints some training information with the model. This should enable\n",
    "    resuming training and having training information on every checkpoint.\n",
    "    Thanks to Roberto Estevao @robertomest - robertomest@poli.ufrj.br\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1, training_args=None, meta=None):\n",
    "\n",
    "        super(MetaCheckpoint, self).__init__(filepath, monitor='val_loss',\n",
    "                                             verbose=0, save_best_only=False,\n",
    "                                             save_weights_only=False,\n",
    "                                             mode='auto', period=1)\n",
    "\n",
    "        self.filepath = filepath\n",
    "        self.meta = meta or {'epochs': []}\n",
    "\n",
    "        if training_args:\n",
    "            self.meta['training_args'] = training_args\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        super(MetaCheckpoint, self).on_train_begin(logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super(MetaCheckpoint, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "        # Get statistics\n",
    "        self.meta['epochs'].append(epoch)\n",
    "        for k, v in logs.items():\n",
    "            # Get default gets the value or sets (and gets) the default value\n",
    "            self.meta.setdefault(k, []).append(v)\n",
    "\n",
    "        # Save to file\n",
    "        filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "\n",
    "        if self.epochs_since_last_save == 0:\n",
    "            with h5py.File(filepath, 'r+') as f:\n",
    "                meta_group = f.create_group('meta')\n",
    "                meta_group.attrs['training_args'] = yaml.dump(\n",
    "                    self.meta.get('training_args', '{}'))\n",
    "                meta_group.create_dataset('epochs',\n",
    "                                          data=np.array(self.meta['epochs']))\n",
    "                for k in logs:\n",
    "                    meta_group.create_dataset(k, data=np.array(self.meta[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('./results/'):\n",
    "    os.mkdir('./results/')\n",
    "checkpoint = MetaCheckpoint('./results/pointnet.h5', monitor='val_acc',\n",
    "                             save_weights_only=True, save_best_only=True,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "class Step(Callback):\n",
    "\n",
    "    def __init__(self, steps, learning_rates, verbose=0):\n",
    "        self.steps = steps\n",
    "        self.lr = learning_rates\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def change_lr(self, new_lr):\n",
    "        old_lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "        if self.verbose == 1:\n",
    "            print('Learning rate is %g' %new_lr)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        for i, step in enumerate(self.steps):\n",
    "            if epoch < step:\n",
    "                self.change_lr(self.lr[i])\n",
    "                return\n",
    "        self.change_lr(self.lr[i+1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'class': type(self).__name__,\n",
    "                  'steps': self.steps,\n",
    "                  'learning_rates': self.lr,\n",
    "                  'verbose': self.verbose}\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        offset = config.get('epoch_offset', 0)\n",
    "        steps = [step - offset for step in config['steps']]\n",
    "        return cls(steps, config['learning_rates'],\n",
    "                   verbose=config.get('verbose', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetenth_50_75(lr):\n",
    "    steps = [50, 75]\n",
    "    lrs = [lr, lr / 10, lr / 100]\n",
    "    return Step(steps, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 458s 7s/step - loss: 3.7932 - acc: 0.1743 - val_loss: 2.9075 - val_acc: 0.2620\n",
      "Epoch 2/100\n",
      " 1/64 [..............................] - ETA: 7:01 - loss: 3.9543 - acc: 0.1250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-cef961e58962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monetenth_50_75\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                               verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train.generator(),\n",
    "                              steps_per_epoch=2048 // batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val.generator(),\n",
    "                              validation_steps=420 // batch_size,\n",
    "                              callbacks=[checkpoint, onetenth_50_75(lr)],\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_meta(model_fname):\n",
    "    ''' Load meta configuration\n",
    "    '''\n",
    "    meta = {}\n",
    "\n",
    "    with h5py.File(model_fname, 'r') as f:\n",
    "        meta_group = f['meta']\n",
    "\n",
    "        meta['training_args'] = yaml.load(\n",
    "            meta_group.attrs['training_args'])\n",
    "        for k in meta_group.keys():\n",
    "            meta[k] = list(meta_group[k])\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.26862785016286644,\n",
       "  0.4483916938110749,\n",
       "  0.5468241042345277,\n",
       "  0.6088151465798045,\n",
       "  0.6486156351791531,\n",
       "  0.6781351791530945,\n",
       "  0.7043973941368078,\n",
       "  0.7199714983713354,\n",
       "  0.7350366449511401,\n",
       "  0.7538680781758957,\n",
       "  0.7553949511400652,\n",
       "  0.7729030944625407,\n",
       "  0.780435667752443,\n",
       "  0.7824714983713354,\n",
       "  0.790207654723127,\n",
       "  0.7892915309446255,\n",
       "  0.8017100977198697,\n",
       "  0.8089372964169381,\n",
       "  0.8131107491856677,\n",
       "  0.8189128664495114,\n",
       "  0.815757328990228,\n",
       "  0.8240024429967426,\n",
       "  0.8209486970684039],\n",
       " 'epochs': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22],\n",
       " 'loss': [3.2427984048955216,\n",
       "  2.206099193336909,\n",
       "  1.7374054249800766,\n",
       "  1.4651202674020773,\n",
       "  1.2962882938136495,\n",
       "  1.1659571400294475,\n",
       "  1.0878957184790012,\n",
       "  1.017467048832182,\n",
       "  0.9487431292036845,\n",
       "  0.882083866757368,\n",
       "  0.868687592616687,\n",
       "  0.8038371513060716,\n",
       "  0.7782100742919438,\n",
       "  0.7640379909972026,\n",
       "  0.7306155241079362,\n",
       "  0.737379225341039,\n",
       "  0.6809870817179788,\n",
       "  0.6579573242303215,\n",
       "  0.6452620600256159,\n",
       "  0.6319839665089834,\n",
       "  0.6342462384739606,\n",
       "  0.6093009219592868,\n",
       "  0.6125563231663906],\n",
       " 'training_args': '{}',\n",
       " 'val_acc': [0.0,\n",
       "  0.49310064935064934,\n",
       "  0.5515422077922078,\n",
       "  0.6566558441558441,\n",
       "  0.713474025974026,\n",
       "  0.6696428571428571,\n",
       "  0.6818181818181818,\n",
       "  0.7357954545454546,\n",
       "  0.7723214285714286,\n",
       "  0.810064935064935,\n",
       "  0.8104707792207793,\n",
       "  0.7832792207792207,\n",
       "  0.7719155844155844,\n",
       "  0.7926136363636364,\n",
       "  0.7905844155844156,\n",
       "  0.8267045454545454,\n",
       "  0.7844967532467533,\n",
       "  0.7905844155844156,\n",
       "  0.8035714285714286,\n",
       "  0.8469967532467533,\n",
       "  0.8226461038961039,\n",
       "  0.8287337662337663,\n",
       "  0.8303571428571429,\n",
       "  0.786525974025974],\n",
       " 'val_loss': [1.9465291871652974,\n",
       "  1.6445186447787594,\n",
       "  1.2205644722108717,\n",
       "  0.999028268572572,\n",
       "  1.1824071716952633,\n",
       "  1.1410545474523073,\n",
       "  0.9324554882266305,\n",
       "  0.780262144742074,\n",
       "  0.6481624686872804,\n",
       "  0.6735739539583008,\n",
       "  0.7002382998342638,\n",
       "  0.7556140066741349,\n",
       "  0.7336837155865384,\n",
       "  0.7648091962585202,\n",
       "  0.6152869141721106,\n",
       "  0.7603146151288763,\n",
       "  0.7082080546911661,\n",
       "  0.6664261372832508,\n",
       "  0.5423338653205277,\n",
       "  0.6369178763457707,\n",
       "  0.6114831779684339,\n",
       "  0.5804787283981001,\n",
       "  0.7178543213900034]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_meta(\"./results/pointnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, result_dir):\n",
    "    plt.plot(history.history['acc'], marker='.')\n",
    "    plt.plot(history.history['val_acc'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend(['acc', 'val_acc'], loc='lower right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_accuracy.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_loss.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, result_dir):\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    nb_epoch = len(acc)\n",
    "\n",
    "    with open(os.path.join(result_dir, 'result.txt'), 'w') as fp:\n",
    "        fp.write('epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n')\n",
    "        for i in range(nb_epoch):\n",
    "            fp.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                i, loss[i], acc[i], val_loss[i], val_acc[i]))\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, './results/')\n",
    "save_history(history, './results/')\n",
    "model.save_weights('./results/pointnet_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from model_cls import PointNet\n",
    "from data_loader import DataGenerator\n",
    "\n",
    "nb_classes = 40\n",
    "test_file = './ModelNet40/ply_data_test.h5'\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "model = PointNet(nb_classes)\n",
    "lr = 0.0001\n",
    "adam = Adam(lr=lr)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "val = DataGenerator(test_file, batch_size, nb_classes, train=False)\n",
    "model.load_weights(\"./results/pointnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-f249dda3ff6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'meta'"
     ]
    }
   ],
   "source": [
    "model.meta.get('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-86b1cc6a0277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'epoch'"
     ]
    }
   ],
   "source": [
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"filename-{}-{}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-05078c6cab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "text.format(epoch=epoch + 1, **logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, l = load_h5(\"./ModelNet40/test/ply_data_test1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
